---
title: "R Notebook"
output: html_notebook
---

# Logistic Regression

binary dependent variable: (0 or 1)

- spam emails or not
- married or not
- clicked or not
- cat or dog
- organic or conventional 
- registered user or not
- accepted or not mortgages


```{r}
library(tidyverse)
library(janitor)
```

```{r}
mortgage_data <- read_csv("data/mortgage_applications.csv") %>%
  clean_names()
              
head(mortgage_data)
```

```{r}
library(GGally)

ggpairs(mortgage_data)
```

```{r}
# Let's reduce the symbol size and 'jitter' the y-values so we can see more of the data without overlap of symbols
# geom_jitter() adds adds a small amount of random variation (vertically and/ore horizontally depending on the arguments) to the location of each point

score_plot <- ggplot(mortgage_data) +
  geom_jitter(aes(y = tu_score, x = as.integer(accepted)), shape = 1, 
              position = position_jitter(width = 0.05)) + 
  xlab("Accepted Status") + scale_x_continuous(breaks=seq(0, 1,1))

score_plot
```

# Logistic Function

Logistic regession is estimating the LOG ODDS OF AN EVENT, RATHER THAN THE PROBABILTITY.

ask - 2 mins
If we roll a fair die, what are the odds of getting a 6?

Odds(success) = 1/6 / 5/6 = 0.2

```{r}
logit <- function(x){
  return(log(x/(1-x))) # x is prob
}

logit_data <- tibble(p = seq(0.001, 0.999, 0.001)) %>%
  mutate(logit_p = logit(p))

head(logit_data)
```

```{r}
ggplot(logit_data, aes(y = p, x = logit_p)) + 
  geom_line() + 
  ylab("probability") + xlab("logit(p) value")
```


**Build a model that predicts the probaility of a loan being accepted or not, given a tu_score**

1. Build the logistis model

```{r}
mortgage_data_log_model <- glm(accepted ~ tu_score, data = mortgage_data, 
                               family = binomial(link = "logit"))

mortgage_data_log_model
```

2. Add these predictions into your dataset

```{r}
library(modelr)

predict_log <- tibble(tu_score = seq(0, 710, 1)) %>% 
  add_predictions(mortgage_data_log_model, type = "response")
```

3. Check model fits the data?
```{r}
ggplot(mortgage_data) + 
  geom_jitter(aes(x = tu_score, y = as.integer(accepted)), shape = 1, 
              position = position_jitter(h = 0.05)) + 
  geom_line(data = predict_log, aes(x = tu_score, y = pred), col = 'red')
```


Task - 5 mins
Use and amend the code above to predict the probability of getting a mortgage application accepted with a tu_score of 594.

```{r}
predict_log_task <- tibble(tu_score = 594) %>% 
  add_predictions(mortgage_data_log_model, type = "response")

predict_log_task
```

60% of people will get a mortgage application accepted 

1/1 + e(b0 = b1 * x)

```{r}
ggplot(mortgage_data) + 
  geom_jitter(aes(x = tu_score, y = as.integer(accepted)), shape = 1, 
              position = position_jitter(h = 0.05)) + 
  geom_point(data = predict_log_task, aes(x = tu_score, y = pred), col = 'red')
```

4. Interpreting the B1 for a continuous predictor (tu_score)

If the independent variable increases by one unit, then the estimate of the log odds of suscces changes (increases or decreases) by B1 units.

Calculate the odds of having an accepted application at tu_score = 594:
```{r}
odds_at_594 <- tibble(tu_score = 594) %>% 
  add_predictions(mortgage_data_log_model, type = "response") %>% 
  mutate(odds = pred / (1-pred))

odds_at_594
```

This implies that a 1 unit increase in tu_score increases the odds of getting approved for a loan by a factor of 1.58.

How do these odds change if we increase the tu_score by 50 points?


```{r}
library(broom)
```

```{r}
mortgage_data_log_model
```
Take the above details and make a table to read it easier below

```{r}
b_tu_score <- tidy(mortgage_data_log_model) %>% 
  filter(term == "tu_score") %>% 
  select(estimate)

b_tu_score
```

```{r}
odds_factor <- exp(b_tu_score * 50)

odds_factor
```

Calculate the new odds of getting a loan accepted or not, based on a 50 point increase in tu_score

```{r}
odds_at_594 <- odds_at_594 %>% 
  select(odds)

odds_at_594
```

```{r}
new_odds <- odds_factor * odds_at_594

new_odds
```

```{r}
odds_at_644 <- tibble(tu_score = 644) %>% 
  add_predictions(mortgage_data_log_model, type = "response") %>% 
  mutate(odds = pred / (1-pred))

odds_at_644
```

Task - 5 mins How do the odds of acceptance change if we decrease tu_score by 50 points from 594 to 544? Calculate the new odds using R.

[Hint - the change in x here is −50] 

```{r}
odds_at_544 <- tibble(tu_score = 544) %>% 
  add_predictions(mortgage_data_log_model, type = "response") %>% 
  mutate(odds = pred / (1-pred))

odds_at_544
```

# Multiple predictors

Make a new model with multiple predictors

```{r}
mortgage_multi_log_model <- glm(accepted ~ tu_score + employed + age, 
                                data = mortgage_data,
                                family = binomial(link = 'logit'))

mortgage_multi_log_model
```




```{r}
tidy_out <- tidy(mortgage_multi_log_model)

tidy_out
```

tu_score and employedTRUE are significant 

Final model: accepted ~ tu_score + employed

```{r}
b_employedTRUE <- tidy(mortgage_multi_log_model) %>% 
  filter(term == "employedTRUE") %>% 
  select(estimate)

exp(b_employedTRUE)
```

On average, a customer's odds of being axxepted for a mortgage are 4.39 times higher if they are employed.

# Evaluating the Perfirmance of a Model


```{r}
mortgage_3pred_model <- glm(accepted ~ tu_score + employed + age, 
                            data = mortgage_data, 
                            family = binomial(link = 'logit'))

mortgage_data_with_3pred <- mortgage_data %>%
  add_predictions(mortgage_3pred_model, type = "response")
head(mortgage_data_with_3pred)
```

**Threshold probability**: cut off for whether a value gets classed as true or false, that depends on a prob level.

Set the threshold to 0.6
```{r}
threshold <-  0.6

mortgage_data_with_3pred <- mortgage_data_with_3pred %>% 
  mutate(pred_thresh_0.6 = pred > threshold)

mortgage_data_with_3pred
```

Count how many times our classifier?model is correct - confusion matrix

```{r}
conf_table <- mortgage_data_with_3pred %>% 
  tabyl(accepted, pred_thresh_0.6)

conf_table
```

The table is pretty self-explanatory:

    The ‘correct’ predictions, i.e. true negatives and true positives are on the top left to bottom right diagonal.
    The ‘incorrect’ predictions, i.e. false negatives and false positives are on the bottom left to top right diagonal. A perfect classifier would have zeroes on this diagonal.

‘True’, ‘false’, ‘negative’ and ‘positive’ can be confusing terms at first. It might help to think that ‘negative’ and ‘positive’ are the outcomes predicted by the logistic regression and ‘true’ and ‘false’ relate to whether the prediction was correct with reference to the sample data.

```{r}
mortgage_data_with_3pred %>% 
  filter(tu_score == 594)
```


# Accuracy Score

What is the accuracy of a classifier

$$\textrm{accuracy} = \frac{NTP + NTN}{N}$$

So, in our case, the classifier is $\frac{679+179}{1000}=0.858$ or $85.8\%$ accurate! This sounds great, but it isn't enough. Accuracy has a **subtle weakness** that means we need to consider additional measures of performance for our classifier.



# Rates

Performance measures of the claaifier/model.

**true positive rate** $$TPR = \frac{NTP}{NTP + NFN}$$

TPR: The number of positive cases that are correctly identified/classified. **Sensitivity** of a classifier.

**true negative rate** $$TNR = \frac{NTN}{NTN + NFP}$$

TNR: The number of negative cases that are correctly identified/classified. **Sensitivity** of a classifier.

**false positive rate** $$FPR = \frac{NFP}{NFP + NTN}$$

FPR: False alarm rate - nagative cases that are incorrectly identified as a positive. **Type I Error**

**false negative rate** $$FNR = \frac{NFN}{NFN + NTP}$$

FNR: incorrectly identifies a case as negative when it's positive :( **Type II Error**


```{r}
NTP <- 179
NTN <- 679
NFP <- 49
NFN <- 93

TPR <- NTP / (NTP + NFN)
TNR <- NTN / (NTN + NFP)
FPR <- NFP / (NFP + NTN)
FNR <- NFN / (NFN + NTP)
```

```{r}
TPR
TNR
FPR
FNR
```

# ROC curves, AUC values, Gini Coefficients and cross validation

```{r}
summary(mortgage_3pred_model)
```

# ROC curves

```{r}
library(pROC)
```

```{r}
roc_obj_3pred <- mortgage_data_with_3pred %>% 
  roc(response = accepted, predictor = pred)
```

```{r}
ggroc(data = roc_obj_3pred, laegacy.axes =TRUE) +
  coord_fixed() +
  ylab("Sensitivy (TPR)") +
  xlab("1-Sensitivy (FPR)")
```


```{r}
classifier_data <- tibble(
  threshold = roc_obj_3pred$thresholds,
  sensitivity = roc_obj_3pred$sensitivities,
  specificity = roc_obj_3pred$specificities
)

head(classifier_data)
```

Task - 10 mins
OK, let’s fit another classifier and add its curve to the ROC plot from earlier!
Fit a single predictor logistic regression model to the `mortgage_data.` We recommend `tu_score` as the predictor. Save the model as `mortgage_1pred_model`
Add the predicted probabilities from this model to `mortgage_data`, and save the resulting data as `mortgage_data_with_1pred`
Use this data to generate an an object from `roc()`, save the object as `roc_obj_1pred`
Pass your old `roc_obj_3pred` and new `roc_obj_1pred` into `ggroc()` [Hint check the ggroc() docs to see how to pass in multiple `roc` objects].
Given these ROC curves, which classifier is better?
If you have time, try another single predictor, i.e. `age` or `employed`

```{r}
mortgage_1pred_model <- glm(accepted ~ tu_score, 
                            data = mortgage_data, 
                            family = binomial(link = 'logit'))

mortgage_data_with_1pred <- mortgage_data %>%
  add_predictions(mortgage_1pred_model, type = "response")
head(mortgage_data_with_1pred)
```

```{r}
threshold <-  0.6

mortgage_data_with_1pred <- mortgage_data_with_1pred %>% 
  mutate(pred_thresh_0.6 = pred > threshold)

mortgage_data_with_1pred
```

```{r}
conf_table_1pred <- mortgage_data_with_1pred %>% 
  tabyl(accepted, pred_thresh_0.6)

conf_table_1pred
```

```{r}
NTP <- 145
NTN <- 684
NFP <- 44
NFN <- 127

TPR <- NTP / (NTP + NFN)
TNR <- NTN / (NTN + NFP)
FPR <- NFP / (NFP + NTN)
FNR <- NFN / (NFN + NTP)
```

```{r}
TPR
TNR
FPR
FNR
```


```{r}
roc_obj_1pred <- mortgage_data_with_1pred %>% 
  roc(response = accepted, predictor = pred)
```

```{r}
ggroc(data = roc_obj_1pred, laegacy.axes =TRUE) +
  coord_fixed() +
  ylab("Sensitivy (TPR)") +
  xlab("1-Sensitivy (FPR)")
```


```{r}
roc_curve <- ggroc(data = list(pred3 = roc_obj_3pred, pred1 = roc_obj_1pred), legacy.axes = TRUE) +
  coord_fixed()

roc_curve
```

ROC - gives us a convienent way to compare performance of classifiers visually

AUC value - gives us a singular number to measure effectiveness of a classifier

AUC : Area under the Curve

```{r}
auc(roc_obj_3pred)
```

```{r}
auc(roc_obj_1pred)
```

Gini Coefficient: normalises AUC so that a total random classifier has 0 and a perfect classifier has 1 : not %

GINI = 2 * AUC -1

0 : 1

```{r}
gini1 <- 2 * 0.866 - 1
gini1

gini3 <- 2 * 0.881 - 1
gini3
```

# Cross Validation

```{r}
library(caret)
```


```{r}
mortgage_data <- mortgage_data %>%
  mutate(employed = as_factor(if_else(employed, "t", "f")),
         accepted = as_factor(if_else(accepted, "t", "f")),
         employed = relevel(employed, ref = "f"),
         accepted = relevel(accepted, ref = "f")) 

mortgage_data
```

```{r}
train_control <- trainControl(method = "repeatedcv",
                              number = 5, # cuts data into 5 folds to test
                              repeats = 100,
                              savePredictions = TRUE,
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary) 


```

```{r}
model <- train(accepted ~ tu_score + employed + age, 
               data = mortgage_data, 
               trControl = train_control, 
               method = "glm", 
               family = binomial(link = "logit"))
```

```{r}
summary(model)
```

```{r}
model$results
```

Ricardo found this:

tem as its discrimination threshold is varied.


A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.
WikipediaWikipedia
Graph of a function
In mathematics, the graph of a function
 
   
     
       f
Show more
https://en.wikipedia.org/wiki/Graph_of_a_function

WikipediaWikipedia
Binary classification
Binary classification is the task of classifying the elements of a set into two groups on the basis of a classification rule. Typical binary classification problems include:
Medical testing to determine if a patient has certain disease or not;
Quality control in industry, deciding whether a specification has been met;
In information retrieval, deciding whether a page should be in the result set of a search or not.Binary classification is dichotomization applied to a practical situation. In many practical binary classification problems, the two groups are not symmetric, and rather than overall accuracy, the relative proportion of different types of errors is of interest. For example, in medical… Show more